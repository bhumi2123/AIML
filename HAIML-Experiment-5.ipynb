{"metadata":{"colab":{"provenance":[]},"coursera":{"schema_names":["AI4MC3-2"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport nltk\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom transformers import *","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LP5Rafny6sRh","executionInfo":{"status":"ok","timestamp":1728068770145,"user_tz":-330,"elapsed":46331,"user":{"displayName":"Ayush Samant","userId":"08964135942703520408"}},"outputId":"5039a037-aa76-4de5-c2d1-33b4583f0713"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n\n  warnings.warn(\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\n\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n\nYou will be able to reuse this secret in all of your notebooks.\n\nPlease note that authentication is recommended but still optional to access public models or datasets.\n\n  warnings.warn(\n\nGroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n\nNo CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n\nTAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"}]},{"cell_type":"code","source":"import util\nfrom util import *","metadata":{"id":"5ruouERuVq53","executionInfo":{"status":"ok","timestamp":1728070138944,"user_tz":-330,"elapsed":466,"user":{"displayName":"Ayush Samant","userId":"08964135942703520408"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\ndata = {\n    \"Report\": [\n        \"mild pulmonary edema, and cardiomegaly...\",\n        \"unremarkable cardiomediastinal silhouette...\",\n        \"lines and tubes are unchanged in position...\",\n        \"postoperative portable film with a right-sided chest tube...\",\n        \"single frontal view of the chest demonstrates no acute abnormality...\"\n    ],\n    \"Cardiomegaly\": [True, False, False, True, False],\n    \"Pulmonary_Edema\": [True, False, False, True, False],\n    \"Pneumonia\": [False, True, True, True, True],\n    \"Pneumothorax\": [False, False, False, False, False],\n    \"Fracture\": [False, False, False, False, False],\n    \"Tube_Present\": [True, False, False, True, False],\n    \"Effusion\": [False, False, False, True, False],\n    \"Consolidation\": [True, False, False, True, False]\n}\n\nprint(\"test_df size: {}\".format(test_df.shape))\ntest_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"0h94kdGkVq57","outputId":"b8624587-b344-4be1-b496-328af8f8cb14","executionInfo":{"status":"ok","timestamp":1728070506238,"user_tz":-330,"elapsed":457,"user":{"displayName":"Ayush Samant","userId":"08964135942703520408"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"test_df size: (5000, 32)\n"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":["   Unnamed: 0             Image  Atelectasis  Cardiomegaly  Consolidation  \\\n","0       28333  00008270_015.png            0             0              0   \n","1       97164  00029855_001.png            1             0              0   \n","2        4236  00001297_000.png            0             0              0   \n","3       42962  00012359_002.png            0             0              0   \n","4       63927  00017951_001.png            0             0              0   \n","\n","   Edema  Effusion  Emphysema  Fibrosis  Hernia  ...  Infiltration_pred  \\\n","0      0         0          0         0       0  ...           0.531233   \n","1      0         1          0         0       0  ...           0.530278   \n","2      0         0          0         0       0  ...           0.253458   \n","3      0         0          0         0       0  ...           0.261658   \n","4      0         0          0         0       0  ...           0.346267   \n","\n","   Mass_pred  Nodule_pred  Atelectasis_pred  Pneumothorax_pred  \\\n","0   0.077701     0.254761          0.071555           0.031860   \n","1   0.316036     0.403122          0.801598           0.793361   \n","2   0.394017     0.414019          0.426408           0.203794   \n","3   0.110398     0.133254          0.061813           0.072169   \n","4   0.060365     0.833480          0.045764           0.038951   \n","\n","   Pleural_Thickening_pred  Pneumonia_pred  Fibrosis_pred  Edema_pred  \\\n","0                 0.125375        0.259928       0.171520    0.005386   \n","1                 0.550383        0.497499       0.176103    0.359178   \n","2                 0.625412        0.371834       0.693987    0.066478   \n","3                 0.104288        0.160986       0.106231    0.088475   \n","4                 0.794928        0.145725       0.697777    0.017091   \n","\n","   Consolidation_pred  \n","0            0.079036  \n","1            0.698990  \n","2            0.258697  \n","3            0.124298  \n","4            0.192074  \n","\n","[5 rows x 32 columns]"],"text/html":["\n","  <div id=\"df-5f6e582f-7506-4def-aad0-4b6111470f7c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Image</th>\n","      <th>Atelectasis</th>\n","      <th>Cardiomegaly</th>\n","      <th>Consolidation</th>\n","      <th>Edema</th>\n","      <th>Effusion</th>\n","      <th>Emphysema</th>\n","      <th>Fibrosis</th>\n","      <th>Hernia</th>\n","      <th>...</th>\n","      <th>Infiltration_pred</th>\n","      <th>Mass_pred</th>\n","      <th>Nodule_pred</th>\n","      <th>Atelectasis_pred</th>\n","      <th>Pneumothorax_pred</th>\n","      <th>Pleural_Thickening_pred</th>\n","      <th>Pneumonia_pred</th>\n","      <th>Fibrosis_pred</th>\n","      <th>Edema_pred</th>\n","      <th>Consolidation_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>28333</td>\n","      <td>00008270_015.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.531233</td>\n","      <td>0.077701</td>\n","      <td>0.254761</td>\n","      <td>0.071555</td>\n","      <td>0.031860</td>\n","      <td>0.125375</td>\n","      <td>0.259928</td>\n","      <td>0.171520</td>\n","      <td>0.005386</td>\n","      <td>0.079036</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>97164</td>\n","      <td>00029855_001.png</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.530278</td>\n","      <td>0.316036</td>\n","      <td>0.403122</td>\n","      <td>0.801598</td>\n","      <td>0.793361</td>\n","      <td>0.550383</td>\n","      <td>0.497499</td>\n","      <td>0.176103</td>\n","      <td>0.359178</td>\n","      <td>0.698990</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4236</td>\n","      <td>00001297_000.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.253458</td>\n","      <td>0.394017</td>\n","      <td>0.414019</td>\n","      <td>0.426408</td>\n","      <td>0.203794</td>\n","      <td>0.625412</td>\n","      <td>0.371834</td>\n","      <td>0.693987</td>\n","      <td>0.066478</td>\n","      <td>0.258697</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>42962</td>\n","      <td>00012359_002.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.261658</td>\n","      <td>0.110398</td>\n","      <td>0.133254</td>\n","      <td>0.061813</td>\n","      <td>0.072169</td>\n","      <td>0.104288</td>\n","      <td>0.160986</td>\n","      <td>0.106231</td>\n","      <td>0.088475</td>\n","      <td>0.124298</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>63927</td>\n","      <td>00017951_001.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.346267</td>\n","      <td>0.060365</td>\n","      <td>0.833480</td>\n","      <td>0.045764</td>\n","      <td>0.038951</td>\n","      <td>0.794928</td>\n","      <td>0.145725</td>\n","      <td>0.697777</td>\n","      <td>0.017091</td>\n","      <td>0.192074</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 32 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f6e582f-7506-4def-aad0-4b6111470f7c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f6e582f-7506-4def-aad0-4b6111470f7c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f6e582f-7506-4def-aad0-4b6111470f7c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-26a25b7b-baf2-43ea-826c-7e0b29040de1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26a25b7b-baf2-43ea-826c-7e0b29040de1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-26a25b7b-baf2-43ea-826c-7e0b29040de1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"test_df"}},"metadata":{}}]},{"cell_type":"markdown","source":"Here are a few example impressions","metadata":{"id":"tPd8dnGW6sRn"}},{"cell_type":"code","source":"# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_labels(sentence_l):\n    \"\"\"\n    Returns a dictionary that indicates presence of each category (from CATEGORIES)\n    in the given sentences.\n    hint: loop over the sentences array and use get_mention_keywords() function.\n\n    Args:\n        sentence_l (array of strings): array of strings representing impression section\n    Returns:\n        observation_d (dict): dictionary mapping observation from CATEGORIES array to boolean value\n    \"\"\"\n    observation_d = {}\n    ### START CODE HERE ###\n    # loop through each category\n    for cat in CATEGORIES:\n\n        # Initialize the observations for all categories to be False\n        observation_d[cat] = False\n\n    # For each sentence in the list:\n    for s in sentence_l: # complete this line\n\n        # Set the characters to all lowercase, for consistent string matching\n        s = s.lower()\n\n        # for each category\n        for cat in CATEGORIES: # complete this line\n\n            # for each phrase that is related to the keyword (use the given function)\n            for phrase in get_mention_keywords(cat): # complete this line\n\n                # make the phrase all lowercase for consistent string matching\n                phrase = phrase.lower()\n\n                # check if the phrase appears in the sentence\n                if phrase in s: # complete this line\n                    observation_d[cat] = True\n\n\n    ### END CODE HERE ###\n    return observation_d","metadata":{"id":"_DKhgVrPVq6D","executionInfo":{"status":"ok","timestamp":1728069550313,"user_tz":-330,"elapsed":447,"user":{"displayName":"Ayush Samant","userId":"08964135942703520408"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"get_f1_table(get_labels, test_df)","metadata":{"id":"Vs6cU5i_Vq6K","outputId":"e2a2ed94-c302-41de-d5b1-39d47d16ce70"},"execution_count":null,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cardiomegaly</td>\n","      <td>0.718</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lung Lesion</td>\n","      <td>0.641</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Airspace Opacity</td>\n","      <td>0.923</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Edema</td>\n","      <td>0.708</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Consolidation</td>\n","      <td>0.270</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Pneumonia</td>\n","      <td>0.369</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atelectasis</td>\n","      <td>0.646</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pneumothorax</td>\n","      <td>0.218</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pleural Effusion</td>\n","      <td>0.722</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Pleural Other</td>\n","      <td>0.667</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Fracture</td>\n","      <td>0.689</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Average</td>\n","      <td>0.597</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Label     F1\n","0       Cardiomegaly  0.718\n","1        Lung Lesion  0.641\n","2   Airspace Opacity  0.923\n","3              Edema  0.708\n","4      Consolidation  0.270\n","5          Pneumonia  0.369\n","6        Atelectasis  0.646\n","7       Pneumothorax  0.218\n","8   Pleural Effusion  0.722\n","9      Pleural Other  0.667\n","10          Fracture  0.689\n","11           Average  0.597"]},"metadata":{}}]},{"cell_type":"code","source":"raw_text = test_df.loc[28, 'Report Impression']\nprint(\"raw text: \\n\\n\" + raw_text)\nprint(\"cleaned text: \\n\\n\" + clean(raw_text))","metadata":{"id":"lGxR7m1x6sRw","outputId":"341e4de6-52a1-483c-f86c-c1230ce05077"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"raw text: \n\n\n\n\n\n \n\n1.  bibasilar opacities, without significant change from the prior \n\nstudy, likely representing bilateral pleural effusions with \n\natelectasis and/or pneumonia.\n\n \n\n2.  interval decrease in focal opacification within the right upper \n\nlung.\n\n \n\n3.  lines and tubes are unchanged. the distal tip of enteric tube \n\nremains coiled within the stomach.\n\n \n\n \n\n\n\ncleaned text: \n\n\n\n1. bibasilar opacities, without significant change from the prior study, likely representing bilateral pleural effusions with atelectasis or pneumonia. 2. interval decrease in focal opacification within the right upper lung. 3. lines and tubes are unchanged. the distal tip of enteric tube remains coiled within the stomach.\n"}]},{"cell_type":"code","source":"get_f1_table(get_labels, test_df, cleanup=True)","metadata":{"id":"EvLEUqT5Vq6N","outputId":"30bee7a4-759c-4847-d785-d72bccaf0831"},"execution_count":null,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cardiomegaly</td>\n","      <td>0.725</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lung Lesion</td>\n","      <td>0.641</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Airspace Opacity</td>\n","      <td>0.922</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Edema</td>\n","      <td>0.707</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Consolidation</td>\n","      <td>0.270</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Pneumonia</td>\n","      <td>0.369</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atelectasis</td>\n","      <td>0.646</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pneumothorax</td>\n","      <td>0.218</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pleural Effusion</td>\n","      <td>0.722</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Pleural Other</td>\n","      <td>0.687</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Fracture</td>\n","      <td>0.689</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Average</td>\n","      <td>0.600</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Label     F1\n","0       Cardiomegaly  0.725\n","1        Lung Lesion  0.641\n","2   Airspace Opacity  0.922\n","3              Edema  0.707\n","4      Consolidation  0.270\n","5          Pneumonia  0.369\n","6        Atelectasis  0.646\n","7       Pneumothorax  0.218\n","8   Pleural Effusion  0.722\n","9      Pleural Other  0.687\n","10          Fracture  0.689\n","11           Average  0.600"]},"metadata":{}}]},{"cell_type":"code","source":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_labels_negative_aware(sentence_l):\n    \"\"\"\n    Returns a dictionary that indicates presence of categories in\n    sentences within the impression section of the report.\n    Only set a label to True if no 'negative words' appeared in the sentence.\n    hint: loop over the sentences array and use get_mention_keywords() function.\n\n    Args:\n        sentence_l (array of strings): array of strings representing impression section\n    Returns:\n        observation_d (dict): dictionary mapping observation from CATEGORIES array to boolean value\n    \"\"\"\n    # Notice that all of the negative words are written in lowercase\n    negative_word_l = [\"no\", \"not\", \"doesn't\", \"does not\", \"have not\", \"can not\", \"can't\", \"n't\"]\n    observation_d = {}\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # Initialize the observation dictionary\n    # so that all categories are not marked present.\n    for cat in CATEGORIES: # complete this line\n\n        # Initialize category to not present.\n        observation_d[cat] = False\n\n    # Loop through each sentence in the list of sentences\n    for s in sentence_l: # complete this line\n\n        # make the sentence all lowercase\n        s = s.lower()\n\n        # Initialize the flag to indicate no negative mentions (yet)\n        negative_flag = False\n\n        # Go through all the negative words in the list\n        for neg in negative_word_l: # complete this line\n\n            # Check if the word is a substring in the sentence\n            if neg in s: # complete this line\n                # set the flag to indicate a negative mention\n                negative_flag = True\n\n                # Once a single negative mention is found,\n                # you can stop checking the remaining negative words\n                break # complete this line\n\n        # When a negative word was not found in the sentence,\n        # check for the presence of the diseases\n        if not negative_flag: # complete this line\n\n            # Loop through the categories list\n            for cat in CATEGORIES: # complete this line\n\n                # Loop through each phrase that indicates this category\n                for phrase in get_mention_keywords(cat): # complete this line\n\n                        # make the phrase all lowercase\n                        phrase = phrase.lower()\n\n                        # Check if the phrase is a substring in the sentence\n                        if phrase in s: # complete this line\n\n                            # Set the observation dictionary\n                            # to indicate the presence of this category\n                            observation_d[cat] = True\n\n\n    ### END CODE HERE ###\n\n    return observation_d","metadata":{"id":"kHHsbWolVq6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Case\")\n\ntest_sentences = [\"Diffuse Reticular pattern, which can be seen with an atypical infection or chronic fibrotic change.\",\n                  \"No Focal Consolidation.\"]\nprint(\"\\nTest Sentences:\\n\")\nfor s in test_sentences:\n    print(s)\n\nprint()\nretrieved_labels = get_labels_negative_aware(test_sentences)\nprint(\"Retrieved labels: \")\n\nfor key, value in sorted(retrieved_labels.items(), key=lambda x: x[0]):\n    print(\"{} : {}\".format(key, value))\nprint()\n\nprint(\"Expected labels: \")\nexpected_labels = {'Cardiomegaly': False, 'Lung Lesion': False, 'Airspace Opacity': True, 'Edema': False, 'Consolidation': False, 'Pneumonia': True, 'Atelectasis': False, 'Pneumothorax': False, 'Pleural Effusion': False, 'Pleural Other': False, 'Fracture': False}\nfor key, value in sorted(expected_labels.items(), key=lambda x: x[0]):\n    print(\"{} : {}\".format(key, value))\nprint()\n\nprint(\"Test Results:\")\nfor category in CATEGORIES:\n    if category not in retrieved_labels:\n        print(f'Category {category} not found in retrieved labels. Please check code.')\n\n    elif retrieved_labels[category] == expected_labels[category]:\n        print(f'Labels match for {category}!')\n\n    else:\n        print(f'Labels mismatch for {category}. Please check code.')","metadata":{"id":"CLtieuQl6sR2","outputId":"e0a13e74-8397-4705-e77b-4a03b623bf87"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Test Case\n\n\n\nTest Sentences:\n\n\n\nDiffuse Reticular pattern, which can be seen with an atypical infection or chronic fibrotic change.\n\nNo Focal Consolidation.\n\n\n\nRetrieved labels: \n\nAirspace Opacity : True\n\nAtelectasis : False\n\nCardiomegaly : False\n\nConsolidation : False\n\nEdema : False\n\nFracture : False\n\nLung Lesion : False\n\nPleural Effusion : False\n\nPleural Other : False\n\nPneumonia : True\n\nPneumothorax : False\n\n\n\nExpected labels: \n\nAirspace Opacity : True\n\nAtelectasis : False\n\nCardiomegaly : False\n\nConsolidation : False\n\nEdema : False\n\nFracture : False\n\nLung Lesion : False\n\nPleural Effusion : False\n\nPleural Other : False\n\nPneumonia : True\n\nPneumothorax : False\n\n\n\nTest Results:\n\nLabels match for Cardiomegaly!\n\nLabels match for Lung Lesion!\n\nLabels match for Airspace Opacity!\n\nLabels match for Edema!\n\nLabels match for Consolidation!\n\nLabels match for Pneumonia!\n\nLabels match for Atelectasis!\n\nLabels match for Pneumothorax!\n\nLabels match for Pleural Effusion!\n\nLabels match for Pleural Other!\n\nLabels match for Fracture!\n"}]},{"cell_type":"markdown","source":"If you implemented this correctly, you should have Consolidation set to **False**, because the test sentence contains \"No Focal Consolidation.  \n\nWith the basic labeling method `get_labels()`, this set Consolidation to True, because it didn't look for 'negative' words.","metadata":{"id":"nL1yKuKd6sR2"}},{"cell_type":"markdown","source":"Check how this changes your aggregate performance:","metadata":{"id":"r72mTZ4TVq6w"}},{"cell_type":"code","source":"get_f1_table(get_labels_negative_aware, test_df, cleanup=True)","metadata":{"id":"ZFUKz7ObVq6x","outputId":"4e192adc-028a-4b53-a000-d51e84e75ec6"},"execution_count":null,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cardiomegaly</td>\n","      <td>0.807</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lung Lesion</td>\n","      <td>0.465</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Airspace Opacity</td>\n","      <td>0.905</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Edema</td>\n","      <td>0.795</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Consolidation</td>\n","      <td>0.424</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Pneumonia</td>\n","      <td>0.463</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atelectasis</td>\n","      <td>0.642</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pneumothorax</td>\n","      <td>0.656</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pleural Effusion</td>\n","      <td>0.859</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Pleural Other</td>\n","      <td>0.621</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Fracture</td>\n","      <td>0.864</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Average</td>\n","      <td>0.682</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Label     F1\n","0       Cardiomegaly  0.807\n","1        Lung Lesion  0.465\n","2   Airspace Opacity  0.905\n","3              Edema  0.795\n","4      Consolidation  0.424\n","5          Pneumonia  0.463\n","6        Atelectasis  0.642\n","7       Pneumothorax  0.656\n","8   Pleural Effusion  0.859\n","9      Pleural Other  0.621\n","10          Fracture  0.864\n","11           Average  0.682"]},"metadata":{}}]},{"cell_type":"code","source":"sampled_test = test_df.sample(200,random_state=0)","metadata":{"id":"vjyK4jdHVq61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negbio_preds = get_negbio_preds(sampled_test)","metadata":{"id":"XRQ9PvZOVq7p","outputId":"8d34d299-d641-4f60-a497-17115efaad89"},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 200/200 [05:04<00:00,  1.52s/it]\n"}]},{"cell_type":"markdown","source":"Next let's calculate the new F1 scores to see the dependency parser does.","metadata":{"id":"yqK4knPx6sSJ"}},{"cell_type":"code","source":"calculate_f1(sampled_test, negbio_preds)","metadata":{"id":"JcSCLZqFVq7r","outputId":"0ff9b8ce-f563-4ab5-e887-10515ac4d344"},"execution_count":null,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cardiomegaly</td>\n","      <td>0.877</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lung Lesion</td>\n","      <td>0.889</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Airspace Opacity</td>\n","      <td>0.952</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Edema</td>\n","      <td>0.808</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Consolidation</td>\n","      <td>0.370</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Pneumonia</td>\n","      <td>0.467</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atelectasis</td>\n","      <td>0.725</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pneumothorax</td>\n","      <td>0.326</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pleural Effusion</td>\n","      <td>0.875</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Pleural Other</td>\n","      <td>0.588</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Fracture</td>\n","      <td>0.783</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Average</td>\n","      <td>0.696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Label     F1\n","0       Cardiomegaly  0.877\n","1        Lung Lesion  0.889\n","2   Airspace Opacity  0.952\n","3              Edema  0.808\n","4      Consolidation  0.370\n","5          Pneumonia  0.467\n","6        Atelectasis  0.725\n","7       Pneumothorax  0.326\n","8   Pleural Effusion  0.875\n","9      Pleural Other  0.588\n","10          Fracture  0.783\n","11           Average  0.696"]},"metadata":{}}]},{"cell_type":"code","source":"basic = get_f1_table(get_labels, sampled_test).rename(columns={\"F1\": \"F1 Basic\"})\nclean_basic = get_f1_table(get_labels, sampled_test, cleanup=True).rename(columns={\"F1\": \"F1 Cleaned\"})\nnegated_basic = get_f1_table(get_labels_negative_aware, sampled_test, cleanup=True).rename(columns={\"F1\": \"F1 Negative Basic\"})\nnegated_negbio = calculate_f1(sampled_test, negbio_preds).rename(columns={\"F1\": \"F1 Negbio\"})\n\njoined_preds = basic.merge(clean_basic, on=\"Label\")\njoined_preds = joined_preds.merge(negated_basic, on=\"Label\")\njoined_preds = joined_preds.merge(negated_negbio,  on=\"Label\")\n\njoined_preds","metadata":{"id":"9rnihcg8Vq7u","outputId":"1f022594-e20e-4b83-b6c0-8fa20b0ef678"},"execution_count":null,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>F1 Basic</th>\n","      <th>F1 Cleaned</th>\n","      <th>F1 Negative Basic</th>\n","      <th>F1 Negbio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Cardiomegaly</td>\n","      <td>0.758</td>\n","      <td>0.771</td>\n","      <td>0.815</td>\n","      <td>0.877</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lung Lesion</td>\n","      <td>0.762</td>\n","      <td>0.762</td>\n","      <td>0.333</td>\n","      <td>0.889</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Airspace Opacity</td>\n","      <td>0.948</td>\n","      <td>0.948</td>\n","      <td>0.922</td>\n","      <td>0.952</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Edema</td>\n","      <td>0.759</td>\n","      <td>0.759</td>\n","      <td>0.771</td>\n","      <td>0.808</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Consolidation</td>\n","      <td>0.260</td>\n","      <td>0.260</td>\n","      <td>0.409</td>\n","      <td>0.370</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Pneumonia</td>\n","      <td>0.359</td>\n","      <td>0.359</td>\n","      <td>0.467</td>\n","      <td>0.467</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atelectasis</td>\n","      <td>0.605</td>\n","      <td>0.605</td>\n","      <td>0.579</td>\n","      <td>0.725</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pneumothorax</td>\n","      <td>0.167</td>\n","      <td>0.167</td>\n","      <td>0.600</td>\n","      <td>0.326</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pleural Effusion</td>\n","      <td>0.730</td>\n","      <td>0.730</td>\n","      <td>0.839</td>\n","      <td>0.875</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Pleural Other</td>\n","      <td>0.588</td>\n","      <td>0.556</td>\n","      <td>0.533</td>\n","      <td>0.588</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Fracture</td>\n","      <td>0.741</td>\n","      <td>0.741</td>\n","      <td>0.889</td>\n","      <td>0.783</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Average</td>\n","      <td>0.607</td>\n","      <td>0.605</td>\n","      <td>0.651</td>\n","      <td>0.696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Label  F1 Basic  F1 Cleaned  F1 Negative Basic  F1 Negbio\n","0       Cardiomegaly     0.758       0.771              0.815      0.877\n","1        Lung Lesion     0.762       0.762              0.333      0.889\n","2   Airspace Opacity     0.948       0.948              0.922      0.952\n","3              Edema     0.759       0.759              0.771      0.808\n","4      Consolidation     0.260       0.260              0.409      0.370\n","5          Pneumonia     0.359       0.359              0.467      0.467\n","6        Atelectasis     0.605       0.605              0.579      0.725\n","7       Pneumothorax     0.167       0.167              0.600      0.326\n","8   Pleural Effusion     0.730       0.730              0.839      0.875\n","9      Pleural Other     0.588       0.556              0.533      0.588\n","10          Fracture     0.741       0.741              0.889      0.783\n","11           Average     0.607       0.605              0.651      0.696"]},"metadata":{}}]},{"cell_type":"markdown","source":"You should see an improvement using the heavier NLP machinery. The F1 for categories, such as Airspace Opacity, Cardiomegaly, Edema, and Pleural Effusion are already fairly high, while others leave something to be desired. To see how you can extend this to improve performance even more, you can take a look at the CheXpert Labeller paper [here]( https://arxiv.org/pdf/1901.07031.pdf).\n\nWhen you're ready, move on to the next section to explore AI techniques for querying raw text.","metadata":{"id":"3OJFfDo36sSK"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"./models\")","metadata":{"id":"vbQEendr6sSO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n</summary>\n<p>\n<ul>\n    <li>The format of the tokens will be <i>[CLS]&lt;question_tokens&gt;[SEP]&lt;answer_tokens&gt;</i></li>\n    <li>To generate a list that repeats an item, such as 'a', you can use ['a'] * 4 to get ['a', 'a', 'a', 'a'].</li>\n    <li>To create padding, generate a list of zeros: [0,0,...0] and add it to the list that you are padding.</li>\n    <li>The number of zeros to pad is the max sequence length minus the length of the list before padding.</li>\n</ul>\n</p>","metadata":{"id":"NH3B3CRd6sSQ"}},{"cell_type":"code","source":"# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef prepare_bert_input(question, passage, tokenizer, max_seq_length=384):\n    \"\"\"\n    Prepare question and passage for input to BERT.\n\n    Args:\n        question (string): question string\n        passage (string): passage string where answer should lie\n        tokenizer (Tokenizer): used for transforming raw string input\n        max_seq_length (int): length of BERT input\n\n    Returns:\n        input_ids (tf.Tensor): tensor of size (1, max_seq_length) which holds\n                               ids of tokens in input\n        input_mask (list): list of length max_seq_length of 1s and 0s with 1s\n                           in indices corresponding to input tokens, 0s in\n                           indices corresponding to padding\n        tokens (list): list of length of actual string tokens corresponding to input_ids\n    \"\"\"\n    # tokenize question\n    question_tokens = tokenizer.tokenize(question)\n\n    # tokenize passage\n    passage_token = tokenizer.tokenize(passage)\n\n    # get special tokens\n    CLS = tokenizer.cls_token\n    SEP = tokenizer.sep_token\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # manipulate tokens to get input in correct form (not adding padding yet)\n    # CLS {question_tokens} SEP {answer_tokens}\n    # This should be a list of tokens\n    tokens = [CLS,*question_tokens,SEP,*passage_token]\n\n\n    # Convert tokens into integer IDs.\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # Create an input mask which has integer 1 for each token in the 'tokens' list\n    input_mask = [1] * len(input_ids)\n\n    # pad input_ids with 0s until it is the max_seq_length\n    # Create padding for input_ids by creating a list of zeros [0,0,...0]\n    # Add the padding to input_ids so that its length equals max_seq_length\n    input_ids = input_ids + ([0] * (max_seq_length - len(input_ids)))\n\n    # Do the same to pad the input_mask so its length is max_seq_length\n    input_mask = input_mask + ([0] * (max_seq_length - len(input_mask)))\n\n    # END CODE HERE\n\n    return tf.expand_dims(tf.convert_to_tensor(input_ids), 0), input_mask, tokens","metadata":{"id":"a2ivYwfIXPHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Expected output\n```CPP\nPassage: My name is Bob.\nQuestion: What is my name?\nTokens:\n['[CLS]', 'What', 'is', 'my', 'name', '?', '[SEP]', 'My', 'name', 'is', 'Bob', '.']\n\nCorresponding input IDs:\ntf.Tensor(\n[[ 101 1327 1110 1139 1271  136  102 1422 1271 1110 3162  119    0    0\n     0    0    0    0    0    0]], shape=(1, 20), dtype=int32)\n\nMask:\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n\n```","metadata":{"id":"LGS4sNub6sST"}},{"cell_type":"code","source":"# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\n    \"\"\"\n    Find start and end indices that maximize sum of start score\n    and end score, subject to the constraint that start is before end\n    and both are valid according to input_mask.\n\n    Args:\n        start_scores (list): contains scores for start positions, shape (1, n)\n        end_scores (list): constains scores for end positions, shape (1, n)\n        input_mask (list): 1 for valid positions and 0 otherwise\n    \"\"\"\n    n = len(start_scores)\n    max_start_i = -1\n    max_end_j = -1\n    max_start_score = -np.inf\n    max_end_score = -np.inf\n    max_sum = -np.inf\n\n    # Find i and j that maximizes start_scores[i] + end_scores[j]\n    # so that i <= j and input_mask[i] == input_mask[j] == 1\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # set the range for i\n    for i in range(n): # complete this line\n\n        # set the range for j\n        for j in range(i, n): #complete this line\n\n            # both input masks should be 1\n            if input_mask[i] == input_mask[j] == 1: # complete this line\n\n                # check if the sum of the start and end scores is greater than the previous max sum\n                if (start_scores[i] + end_scores[j]) > max_sum: # complete this line\n\n                    # calculate the new max sum\n                    max_sum = start_scores[i] + end_scores[j]\n\n                    # save the index of the max start score\n                    max_start_i = i\n\n                    # save the index for the max end score\n                    max_end_j = j\n\n                    # save the value of the max start score\n                    max_start_val = start_scores[i]\n\n                    # save the value of the max end score\n                    max_end_val = end_scores[j]\n\n    ### END CODE HERE ###\n    if verbose:\n        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\n        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\n        print(f\"max start + max end sum of scores is {max_sum}\")\n    return max_start_i, max_end_j","metadata":{"id":"d66oFFx1aXIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\nend_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\ninput_mask = [1, 1, 1, 1, 1, 0, 0, 0]\n\nstart, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\n\nprint(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"FUQGHgir5wwl","outputId":"2c540701-7b55-47dd-db58-15d601e83c8b"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"max start is at index i=1 and score 2.0\n\nmax end is at index i=4 and score 4.0\n\nmax start + max end sum of scores is 6.0\n\nExpected: (1, 4) \n\nReturned: (1, 4)\n"}]},{"cell_type":"markdown","source":"##### Expected output\n\n```CPP\nmax start is at index i=1 and score 2.0\nmax end is at index i=4 and score 4.0\nmax start + max end sum of scores is 6.0\nExpected: (1, 4)\nReturned: (1, 4)\n```","metadata":{"id":"6uRe3KaN6sSV"}},{"cell_type":"code","source":"# Test 2\n\nstart_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\nend_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\ninput_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\n\nstart, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\n\nprint(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))","metadata":{"id":"0lmLtTpN6sSW","outputId":"27f795c7-5837-4ab3-ec61-144a851b6a8f"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"max start is at index i=1 and score 2.0\n\nmax end is at index i=1 and score 5.0\n\nmax start + max end sum of scores is 7.0\n\nExpected: (1, 1) \n\nReturned: (1, 1)\n"}]},{"cell_type":"markdown","source":"##### Expected output\n```CPP\nmax start is at index i=1 and score 2.0\nmax end is at index i=1 and score 5.0\nmax start + max end sum of scores is 7.0\nExpected: (1, 1)\nReturned: (1, 1)\n```\nIf your expected output differes in this second test, please check how you set the range of your for loops.","metadata":{"id":"gsWvu4sq6sSW"}},{"cell_type":"code","source":"# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef construct_answer(tokens):\n    \"\"\"\n    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\n    Args:\n        tokens: a list of tokens (strings)\n\n    Returns:\n        out_string: the processed string.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # join the tokens together with whitespace\n    out_string = ' '.join(tokens)\n\n    # replace ' ##' with empty string\n    out_string = re.sub(' ##','',out_string)\n\n    # remove leading and trailing whitespace\n    out_string = out_string.strip()\n\n    ### END CODE HERE ###\n\n    # if there is an '@' symbol in the tokens, remove all whitespace\n    if '@' in tokens:\n        out_string = out_string.replace(' ', '')\n\n    return out_string","metadata":{"id":"Ab8Wmdit6sSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n\ntmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\ntmp_out_string_1 = construct_answer(tmp_tokens_1)\n\nprint(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\n\n\ntmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\ntmp_out_string_2 = construct_answer(tmp_tokens_2)\nprint(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\n","metadata":{"id":"G8Uks9iR6sSX","outputId":"bd35d07a-5258-4e96-f6d0-668564dedbf2"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"tmp_out_string_1: hello how  are  you?, length 20\n\ntmp_out_string_2: @hellohowareyou?, length 16\n"}]},{"cell_type":"markdown","source":"##### Expected output\n```CPP\ntmp_out_string_1: hello how  are  you?, length 20\ntmp_out_string_2: @hellohowareyou?, length 16\n```","metadata":{"id":"E8jpkwr56sSY"}},{"cell_type":"code","source":"model = TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")","metadata":{"id":"qEpeFNIq6sSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\n    \"\"\"\n    Identify answer in passage for a given question using BERT.\n\n    Args:\n        model (Model): pretrained Bert model which we'll use to answer questions\n        question (string): question string\n        passage (string): passage string\n        tokenizer (Tokenizer): used for preprocessing of input\n        max_seq_length (int): length of input for model\n\n    Returns:\n        answer (string): answer to input question according to model\n    \"\"\"\n    # prepare input: use the function prepare_bert_input\n    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\n\n    # get scores for start of answer and end of answer\n    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\n    # pass in in the input ids that are returned by prepare_bert_input\n    start_scores, end_scores = model(input_ids)\n\n    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\n    # To pass these into get_span_from_scores function,\n    # take the value at index 0 to get a tensor of shape [max_seq_length]\n    start_scores = start_scores[0]\n    end_scores = end_scores[0]\n\n    # using scores, get most likely answer\n    # use the get_span_from_scores function\n    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\n\n    # Using array indexing to get the tokens from the span start to span end (including the span_end)\n    answer_tokens = tokens[span_start:span_end+1]\n\n    # Combine the tokens into a single string and perform post-processing\n    # use construct_answer\n    answer = construct_answer(answer_tokens)\n\n    return answer","metadata":{"id":"W0BT_u8M6sSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passage = \"Computational complexity theory is a branch of the theory \\\n           of computation in theoretical computer science that focuses \\\n           on classifying computational problems according to their inherent \\\n           difficulty, and relating those classes to each other. A computational \\\n           problem is understood to be a task that is in principle amenable to \\\n           being solved by a computer, which is equivalent to stating that the \\\n           problem may be solved by mechanical application of mathematical steps, \\\n           such as an algorithm.\"\n\nquestion = \"What branch of theoretical computer science deals with broadly \\\n            classifying computational problems by difficulty and class of relationship?\"\n\nprint(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\nprint(\"Expected: Computational complexity theory\")","metadata":{"id":"WaLTKMuw6sSd","outputId":"1d6ce075-281d-42f8-9c26-4d4e0895900f"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Output: Computational complexity theory\n\nExpected: Computational complexity theory\n"}]},{"cell_type":"code","source":"passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\n           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\n           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\n           advice and a range of services that are now performed solely by other specialist practitioners, \\\n           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\n           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\n           Often the place that did this was called an apothecary and several languages have this as the \\\n           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\n           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\n           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\n           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\n\nquestion = \"What word is the word pharmacy taken from?\"\n\nprint(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\nprint(\"Expected: pharma\")","metadata":{"id":"lk9snecF6sSd","outputId":"1a877cf2-99bc-4ae2-daba-ee4b9f5aa68c"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Output: pharma\n\nExpected: pharma\n"}]},{"cell_type":"markdown","source":"Now let's try it on clinical notes. Below we have an excerpt of a doctor's notes for a patient with an abnormal echocardiogram (this sample is taken from [here](https://www.mtsamples.com/site/pages/sample.asp?Type=6-Cardiovascular%20/%20Pulmonary&Sample=1597-Abnormal%20Echocardiogram))","metadata":{"id":"SaDKMoEj6sSe"}},{"cell_type":"code","source":"passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\n           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\n           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\n           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\n           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\n           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\n           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\n           for many years. She also was told that she has a heart murmur, which was not followed through \\\n           on a regular basis.\"\n\nq1 = \"How old is the patient?\"\nq2 = \"Does the patient have any complaints?\"\nq3 = \"What is the reason for this consultation?\"\nq4 = \"What does her echocardiogram show?\"\nq5 = \"What other symptoms does the patient have?\"\n\n\nquestions = [q1, q2, q3, q4, q5]\n\nfor i, q in enumerate(questions):\n    print(\"Question {}: {}\".format(i+1, q))\n    print()\n    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\n    print()\n    print()","metadata":{"id":"vdgCH3hf6sSf","outputId":"8da6e257-3a4e-4e79-d4bd-b11d7b113882"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Question 1: How old is the patient?\n\n\n\nAnswer: 86\n\n\n\n\n\nQuestion 2: Does the patient have any complaints?\n\n\n\nAnswer: The patient complains of shortness of breath\n\n\n\n\n\nQuestion 3: What is the reason for this consultation?\n\n\n\nAnswer: further evaluation\n\n\n\n\n\nQuestion 4: What does her echocardiogram show?\n\n\n\nAnswer: severe mitral regurgitation and also large pleural effusion\n\n\n\n\n\nQuestion 5: What other symptoms does the patient have?\n\n\n\nAnswer: colitis and also diverticulitis\n\n\n\n\n"}]}]}